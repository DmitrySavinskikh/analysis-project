{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do web scrapping of afisha.ru: we need to collect info about restaurants in Moscow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Firstly, we'll collect the restaurants links by going through all the pages with Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(7636) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(7677) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m     next_button \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPagination_pagination-next__rtqsZ\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;66;03m#try to click next page button\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     next_button\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m---> 22\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.afisha.ru\"\n",
    "url = f\"{base_url}/msk/restaurants/restaurant_list/\"\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "driver.get(url)\n",
    "time.sleep(7)\n",
    "\n",
    "all_links = set()  #We need to use set to avoid repeat links\n",
    "\n",
    "while True:\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    for a in soup.find_all(\"a\"):\n",
    "        if \"href\" in a.attrs and a[\"href\"].startswith(\"/msk/restaurant\"): #Search only links starts with /msk/restaurant\n",
    "            all_links.add(a[\"href\"])\n",
    "\n",
    "    try:\n",
    "        next_button = driver.find_element(By.CLASS_NAME, 'Pagination_pagination-next__rtqsZ')#try to click next page button\n",
    "        next_button.click()\n",
    "        time.sleep(5)\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            close_button = driver.find_element(By.CLASS_NAME, 'popmechanic-close')#unexpectedly a banner pops up on the site, try to close it, as it covers the usual page layout\n",
    "            close_button.click()\n",
    "            time.sleep(2)\n",
    "\n",
    "            next_button = driver.find_element(By.CLASS_NAME, 'Pagination_pagination-next__rtqsZ')#try to click next page button again\n",
    "            next_button.click()\n",
    "            time.sleep(5)\n",
    "        except Exception as inner_e:\n",
    "            print(\"There are no more pages:\", inner_e)\n",
    "            break\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We saved 11957 links in 'rest_links.csv'\n"
     ]
    }
   ],
   "source": [
    "#adding base url to links\n",
    "full_links = [base_url + link for link in all_links]\n",
    "all_links_list = list(full_links)\n",
    "\n",
    "df = pd.DataFrame(all_links_list, columns=['Link'])\n",
    "df.to_csv('resto_links.csv', index=False)\n",
    "\n",
    "print(f\"We saved {len(all_links_list)} links in 'rest_links.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "with open('data.csv', 'w', encoding='utf-8-sig', newline='') as d:\n",
    "    file_writer =  csv.writer(d, delimiter = \";\")\n",
    "    file_writer.writerow([\"название ресторана\", \"оценка\",\"адрес\", \"метро\", \"средний чек\", \"завтраки\",\"бизнес-ланч\", \"доставка\",\"парковка\",\"кейтеринг\",\"банкеты\",\"телефон\", \"сайт\", \"тип заведения\",\"открыто\"]) \n",
    "\n",
    "with open('resto_links.csv','r') as f:\n",
    "    all_links = f.readlines()\n",
    "for link in all_links:\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(link)\n",
    "    time.sleep(7)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        \n",
    "    name = soup.find('span',class_ = \"Title_header__SIloF\").text\n",
    "    #bill = soup.find('div',class_ = \"RestaurantExtraInfo_table__l34_J\").find('div',class_ = \"RestaurantExtraInfo_table-cell__fk6SJ\").find('div',class_ = \"RestaurantExtraInfo_cell-data__3Xfkk\").text\n",
    "    rank = soup.find('div',class_ = \"RestaurantCover_rating-wrapper__CTNts\")\n",
    "    if rank!=None:\n",
    "        estimate = rank.text\n",
    "    else:\n",
    "        estimate = 0\n",
    "    adress = soup.find('div', class_=\"SectionTitle_wrapper__nAAJ0 RestaurantExtraInfo_address__aJsK2\").text\n",
    "    metro = soup.find_all('ul', class_=\"RestaurantExtraInfo_metro-list__KTBX3\")\n",
    "    metro_=[]\n",
    "    for i in metro:\n",
    "        for j in i.find_all('span', class_=\"Text_text__e9ILn\"):\n",
    "            metro_ += [j.text]\n",
    "    s = ''       \n",
    "    all = soup.find_all('div',class_ = \"RestaurantExtraInfo_table__l34_J\")\n",
    "    for i in all:\n",
    "        for j in i.find_all('span', class_=\"Text_text__e9ILn\"):\n",
    "           s += j.text + ','\n",
    "        tot =s.split(',')[1:len(s):2]   \n",
    "    bill = tot[0] \n",
    "    brekfast = tot[1]\n",
    "    business =  tot[2]\n",
    "    delivery = tot[3]\n",
    "    parking = tot[4]\n",
    "    keit = tot[5]\n",
    "    feast = tot[6] \n",
    "    phone_num = tot[7]\n",
    "    site_ = tot[8] \n",
    "    type_all = []\n",
    "    type_ = soup.find_all('div',class_ = \"RestaurantExtraInfo_tag__BqQ7e\")\n",
    "    for i in type_:\n",
    "        for j in i.find_all('div', class_=\"Button_button__content___D2b_\"):\n",
    "            type_all +=[j.text] \n",
    "    work = soup.find_all('div',class_ = \"RestaurantCover_content-wrapper__72Dox\")\n",
    "    for i in work:\n",
    "        for j in i.find_all('span', class_=\"Text_text__e9ILn\"):\n",
    "            e = j          \n",
    "    open_ = e.text.strip('Открыто c')    \n",
    "\n",
    "    #good = soup.find_all('div',class_ = \"Button_button__content___D2b_\")\n",
    "    #print(link)\n",
    "    #for i in good:\n",
    "        #for j in i.find_all('span', class_=\"DefaultTag_button__counter__64UpQ\"): \n",
    "            #good1 = j.text\n",
    "                  \n",
    "    with open('data.csv', 'a', encoding='utf-8-sig', newline='') as d:\n",
    "        file_writer =  csv.writer(d, delimiter = \";\")\n",
    "        file_writer.writerow([name,estimate,  adress, metro_, bill, brekfast ,business ,delivery ,parking ,keit ,feast ,phone_num , site_, type_all, open_])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
