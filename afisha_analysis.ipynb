{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do web scrapping of afisha.ru: we need to collect info about restaurants in Moscow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Firstly, we'll collect the restaurants links by going through all the pages with Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.afisha.ru\"\n",
    "url = f\"{base_url}/msk/restaurants/restaurant_list/\"\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "driver.get(url)\n",
    "time.sleep(7)\n",
    "\n",
    "all_links = set()  #We need to use set to avoid repeat links\n",
    "\n",
    "while True:\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    for a in soup.find_all(\"a\"):\n",
    "        if \"href\" in a.attrs and a[\"href\"].startswith(\"/msk/restaurant\"): #Search only links starts with /msk/restaurant\n",
    "            all_links.add(a[\"href\"])\n",
    "\n",
    "    try:\n",
    "        next_button = driver.find_element(By.CLASS_NAME, 'Pagination_pagination-next__rtqsZ')#try to click next page button\n",
    "        next_button.click()\n",
    "        time.sleep(5)\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            close_button = driver.find_element(By.CLASS_NAME, 'popmechanic-close')#unexpectedly a banner pops up on the site, try to close it, as it covers the usual page layout\n",
    "            close_button.click()\n",
    "            time.sleep(2)\n",
    "\n",
    "            next_button = driver.find_element(By.CLASS_NAME, 'Pagination_pagination-next__rtqsZ')#try to click next page button again\n",
    "            next_button.click()\n",
    "            time.sleep(5)\n",
    "        except Exception as inner_e:\n",
    "            print(\"There are no more pages:\", inner_e)\n",
    "            break\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collected links are combined into a DataFrame to be shared with other group members for gathering information about restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We saved 11957 links in 'rest_links.csv'\n"
     ]
    }
   ],
   "source": [
    "#adding base url to links\n",
    "full_links = [base_url + link for link in all_links]\n",
    "all_links_list = list(full_links)\n",
    "\n",
    "df = pd.DataFrame(all_links_list, columns=['Link'])\n",
    "df.to_csv('resto_links.csv', index=False)\n",
    "\n",
    "print(f\"We saved {len(all_links_list)} links in 'rest_links.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To collect information about restaurants in Moscow, we selected the following characteristics:\n",
    "\n",
    "* title: name of the restaurant\n",
    "* rating: the establishment's rating on the website\n",
    "* address: restaurant address\n",
    "* metro: the nearest metro station to the restaurant\n",
    "* avg_check: average check at the establishment\n",
    "* breakfast: availability of a breakfast menu\n",
    "* business_lunch: availability of a business lunch menu\n",
    "* delivery: availability of delivery services\n",
    "* parking: availability of parking\n",
    "* catering: availability of catering services\n",
    "* banquets: whether banquets are held\n",
    "* telephone: restaurant phone number\n",
    "* site: restaurant website\n",
    "* restaurant_type: type of establishment or cuisine\n",
    "* open_hours: opening hours\n",
    "* positive_reviews: number of positive reviews on the website\n",
    "* negative_reviews: number of negative reviews on the website\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We extract the required tags and their content from each page and then form a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for collecting information about restaurants reviews\n",
    "def get_reviews_count(button):\n",
    "    content_div = button.find('div', class_='Button_button__content___D2b_')\n",
    "    if content_div:\n",
    "        counter_span = content_div.find('span', class_='DefaultTag_button__counter__64UpQ')\n",
    "        if counter_span:\n",
    "            return int(counter_span.text)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "with open('resto_links.csv', 'w', encoding='utf-8-sig', newline='') as d:\n",
    "    file_writer =  csv.writer(d, delimiter = \";\")\n",
    "    file_writer.writerow([\"title\", \"rating\", \"address\", \"metro\", \"avg_check\", \"breakfast\", \"business_lunch\", \"deleviry\", \"parking\", \"catering\", \"banquets\", \"telephone\", \"site\", \"restaurant_type\", \"open_hours\", \"positive_reviews\", \"negative_reviews\"]) \n",
    "\n",
    "with open('for_vera_2.csv','r') as f:\n",
    "    all_links = f.readlines()\n",
    "\n",
    "# parsing data with progress bar and remaining time\n",
    "for i, link in enumerate(tqdm(all_links, desc=\"Links processing\", unit=\"link\", dynamic_ncols=True)):\n",
    "    try:\n",
    "\n",
    "        driver.get(link)\n",
    "        driver.implicitly_wait(10)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "            \n",
    "        name = soup.find('span',class_ = \"Title_header__SIloF\").text\n",
    "        rank = soup.find('div',class_ = \"RestaurantCover_rating-wrapper__CTNts\")\n",
    "        if rank!=None:\n",
    "            estimate = rank.text\n",
    "        else:\n",
    "            estimate = 0\n",
    "        adress = soup.find('div', class_=\"SectionTitle_wrapper__nAAJ0 RestaurantExtraInfo_address__aJsK2\").text\n",
    "        metro = soup.find_all('ul', class_=\"RestaurantExtraInfo_metro-list__KTBX3\")\n",
    "        metro_=[]\n",
    "        for i in metro:\n",
    "            for j in i.find_all('span', class_=\"Text_text__e9ILn\"):\n",
    "                metro_ += [j.text]\n",
    "        s = ''       \n",
    "        all = soup.find_all('div',class_ = \"RestaurantExtraInfo_table__l34_J\")\n",
    "        for i in all:\n",
    "            for j in i.find_all('span', class_=\"Text_text__e9ILn\"):\n",
    "                s += j.text + ','\n",
    "            tot =s.split(',')[1:len(s):2]   \n",
    "        bill = tot[0] \n",
    "        brekfast = tot[1]\n",
    "        business =  tot[2]\n",
    "        delivery = tot[3]\n",
    "        parking = tot[4]\n",
    "        keit = tot[5]\n",
    "        feast = tot[6] \n",
    "        phone_num = tot[7]\n",
    "        site_ = tot[8] \n",
    "        type_all = []\n",
    "        type_ = soup.find_all('div',class_ = \"RestaurantExtraInfo_tag__BqQ7e\")\n",
    "        for i in type_:\n",
    "            for j in i.find_all('div', class_=\"Button_button__content___D2b_\"):\n",
    "                type_all +=[j.text] \n",
    "        work = soup.find_all('div',class_ = \"RestaurantCover_content-wrapper__72Dox\")\n",
    "        for i in work:\n",
    "            for j in i.find_all('span', class_=\"Text_text__e9ILn\"):\n",
    "                e = j          \n",
    "        open_ = e.text.strip('Открыто c')    \n",
    "\n",
    "        filters_div = soup.find('div', class_='FiltersReview_filters__7E8qs')\n",
    "        positive_reviews = 0\n",
    "        negative_reviews = 0\n",
    "\n",
    "        if filters_div:\n",
    "            buttons = filters_div.find_all('button', class_='Button_button__j_Rc9')\n",
    "            for button in buttons:\n",
    "                content_div = button.find('div', class_='Button_button__content___D2b_')\n",
    "                if content_div:\n",
    "                    text = content_div.text\n",
    "                    if 'Положительные' in text:\n",
    "                        positive_reviews = get_reviews_count(button)\n",
    "                    elif 'Отрицательные' in text:\n",
    "                        negative_reviews = get_reviews_count(button)\n",
    "\n",
    "                    \n",
    "        with open('data.csv', 'a', encoding='utf-8-sig', newline='') as d:\n",
    "            file_writer =  csv.writer(d, delimiter = \";\")\n",
    "            file_writer.writerow([name, estimate, adress, metro_, bill, brekfast ,business ,delivery ,parking ,keit ,feast ,phone_num , site_, type_all, open_, positive_reviews, negative_reviews])\n",
    "\n",
    "    except TypeError:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"Link processing error {link}: {e}\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating DataFrame\n",
    "Using CSV-file with whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'big_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbig_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers.py:610\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    605\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    606\u001b[0m     dialect, delimiter, delim_whitespace, engine, sep, defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    607\u001b[0m )\n\u001b[1;32m    608\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers.py:462\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    459\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    461\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers.py:819\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 819\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers.py:1050\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1047\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown engine: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (valid options are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1048\u001b[0m     )\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;66;03m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[0;32m-> 1050\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers.py:1867\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1864\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# open handles\u001b[39;00m\n\u001b[0;32m-> 1867\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers.py:1362\u001b[0m, in \u001b[0;36mParserBase._open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_handles\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: FilePathOrBuffer, kwds: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;124;03m    Let the readers open IOHanldes after they are done with their potential raises.\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/common.py:642\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m         errors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 642\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'big_data.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('big_data.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing rating column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_rating_column(\n",
    "        data_frame: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    '''\n",
    "        There are cells with the pattern \"n отзывов\", they are replaced by '0'.\n",
    "        The rating column is converted to the float (numeric) type.\n",
    "        The average value of the review is set to 0.\n",
    "\n",
    "        :param data_frame: pd.DataFrame\n",
    "\n",
    "        :return: pd.DataFrame\n",
    "    '''\n",
    "    data_frame.loc[data_frame['rating'].str.contains('отз'), 'rating'] = '0'\n",
    "    data_frame['rating'] = pd.to_numeric(data_frame['rating'])\n",
    "\n",
    "    mean_rating = round(data_frame['rating'][data_frame['rating'] != 0.0].mean(), 2)\n",
    "    data_frame['rating'] = np.where(data_frame['rating'] == 0.0, mean_rating, data_frame['rating'])\n",
    "\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "df = fix_rating_column(data_frame=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_information_columns(\n",
    "        data_frame: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    '''\n",
    "        Changing breakfast, business_lunch, deleviry, parking, catering, banquets columns\n",
    "        to 0 or 1. Added column is_city_center (0 or 1) to see is restaurant in garden ring in Moscow.\n",
    "\n",
    "        :param data_frame: pd.DataFrame\n",
    "\n",
    "        :return: pd.DataFrame\n",
    "    '''\n",
    "    station_metro = 'Цветной бульварТверскаяЧеховскаяБоровицкаяаяПолянкаДобрынинскаяСерпуховскаяМаяковскаяТеатральнаяНовокузнецкаяПавелецкаяПушкинскаяКузнецкий мостКитай-городТаганскаяМарксистскаяСмоленскаяАрбатскаяАлександровский садПлощадь РеволюцииЧкаловскаяКурскаяПарк культурыКропоткинскаяБиблиотека им. ЛенинаОхотный РядЛубянкаКрасные воротаЧистые прудыКрасные воротаОктябрьскаяТретьяковскаяТургеневскаяСухаревскаяПроспект МираТрубнаяСретенский бульвар' \n",
    "    data_frame['breakfast'] = (data_frame['breakfast'] =='Есть').astype(int)\n",
    "    data_frame['business_lunch'] = (data_frame['business_lunch'] =='Есть').astype(int)\n",
    "    data_frame['deleviry'] = (data_frame['deleviry'] =='Есть').astype(int)\n",
    "    data_frame['parking'] = (data_frame['parking'] =='Есть').astype(int)\n",
    "    data_frame['catering'] = (data_frame['catering'] =='Есть').astype(int)\n",
    "    data_frame['banquets'] = (data_frame['banquets'] =='Есть').astype(int)\n",
    "\n",
    "    sites=list(data_frame['site'].values)\n",
    "    sites1=[]\n",
    "    for i in range(len(sites)):\n",
    "        if sites[i].startswith('http://'):\n",
    "            sites1+=[1]\n",
    "        else:\n",
    "            sites1+=[0]       \n",
    "\n",
    "    data_frame['site'] = sites1\n",
    "\n",
    "    telephones=list(data_frame['telephone'].values)\n",
    "    telephone1=[]\n",
    "    for i in range(len(telephones)):\n",
    "        if telephones[i].startswith('+7'):\n",
    "            telephone1+=[1]\n",
    "        else:\n",
    "            telephone1+=[0]   \n",
    "    data_frame['telephone'] = telephone1     \n",
    "\n",
    "    #data_frame.loc[data_frame['avg_check'] =='Нет', 'avg_check'] = 0   \n",
    "\n",
    "    avg = list(data_frame['avg_check'].values)\n",
    "    avg1=[]\n",
    "    for i in range(len(avg)):\n",
    "        if avg[i].startswith('До 7'):\n",
    "            avg1+=[1]\n",
    "        elif avg[i].startswith('70'):\n",
    "            avg1+=[2]\n",
    "        elif avg[i].startswith('170'):\n",
    "            avg1+=[3]\n",
    "        elif avg[i].startswith('Бо'):\n",
    "            avg1+=[4]        \n",
    "        else:\n",
    "            avg1+=[0]   \n",
    "    data_frame['avg_check'] = avg1\n",
    "    metros = list(data_frame['metro'].values)\n",
    "    metro1=[]\n",
    "    metro2=[]\n",
    "    for i in metros:\n",
    "        metro1 += [i[2:len(i)-2].split(',')]\n",
    "    for i in range(len(metro1)):\n",
    "        if metro1[i][0] in station_metro:\n",
    "            metro2 +=[1]\n",
    "        else:\n",
    "            metro2 +=[0] \n",
    "    data_frame['is_city_center']=metro2     \n",
    "\n",
    "    return data_frame\n",
    "\n",
    "df = transform_information_columns(data_frame=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating open_hours column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_opened_dates(\n",
    "        data_frame: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    '''\n",
    "        There is \"hh:mm до hh:mm\" format in open_hours column.\n",
    "        It need to separate this column to opened_from_dttm and opened_to_dttm\n",
    "        and correct mistakes.\n",
    "\n",
    "        :param data_frame: pd.DataFrame\n",
    "\n",
    "        :return: pd.DataFrame\n",
    "    '''\n",
    "\n",
    "    for index, row in data_frame.iterrows():\n",
    "        if ' до ' not in row['open_hours']:\n",
    "            data_frame.loc[index, 'open_hours'] = '10:00 до 22:00' # change cells with mistakes to usual open-close time '10:00 до 22:00'\n",
    "    \n",
    "    for index, row in data_frame.iterrows():\n",
    "        open_from_to = row['open_hours'].split(' до ')\n",
    "        data_frame.loc[index, 'opened_from_dttm'] = datetime.time(datetime.strptime(open_from_to[0], '%H:%M'))\n",
    "        data_frame.loc[index, 'opened_to_dttm'] = datetime.time(datetime.strptime(open_from_to[1], '%H:%M'))\n",
    "    \n",
    "    data_frame = data_frame.drop(columns=['open_hours'])\n",
    "    return data_frame\n",
    "    \n",
    "\n",
    "df = separate_opened_dates(data_frame=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting review and finding good restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_review(\n",
    "        data_frame: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    '''\n",
    "        If there are more positive reviews than negative ones \n",
    "        and the rating is more than 8, \n",
    "        then write 1 to the new top_restaurant column.\n",
    "\n",
    "        :param data_frame: pd.DataFrame\n",
    "\n",
    "        :return: pd.DataFrame\n",
    "    '''\n",
    "\n",
    "    top_rest = []\n",
    "    pos = data_frame['positive_reviews'].values\n",
    "    neg = data_frame['negative_reviews'].values\n",
    "    ran = data_frame['rating'].values\n",
    "\n",
    "    for i in range(len(pos)):\n",
    "        if (pos[i]>=neg[i]) and ran[i]>=8:\n",
    "            top_rest += [1]\n",
    "        else:\n",
    "            top_rest += [0]  \n",
    "    data_frame['top_restaurant']= top_rest\n",
    "    return data_frame\n",
    "\n",
    "df = sorting_review(data_frame=df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
